{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 sentences in training data\n",
      "13  documents\n",
      "3  classes ['where_is', 'schedule_list', 'greetings']\n",
      "38  unique stemmed words ['direct', 'can', 'schedule', 'you', 'hav', 'my', 'what', 'how', 'morn', 'sund', 'lot', 'to', 'next', 'tim', 'nic', 'day', 'ar', 'good', 'giv', 'narkel', 'show', 'a', 'bag', 'meet', 'hi', 'wher', '.', 'way', 'is', 'park', 'sess', 'staircas', 'sast', 'me', 'pleas', 'when', 'the', 'doing']\n",
      "['what', 'tim', 'is', 'my', 'next', 'sess', '?']\n",
      "[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# use natural language toolkit\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import os\n",
    "import json\n",
    "import datetime\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# 3 classes of training data\n",
    "training_data = []\n",
    "training_data.append({\"class\":\"schedule_list\", \"sentence\":\"What time is my next session?\"})\n",
    "training_data.append({\"class\":\"schedule_list\", \"sentence\":\"When is my next session?\"})\n",
    "training_data.append({\"class\":\"schedule_list\", \"sentence\":\"What time is my next meeting?\"})\n",
    "training_data.append({\"class\":\"schedule_list\", \"sentence\":\"Can you please show me my schedule?\"})\n",
    "training_data.append({\"class\":\"schedule_list\", \"sentence\":\"Show me my schedule.\"})\n",
    "\n",
    "training_data.append({\"class\":\"greetings\", \"sentence\":\"Hi\"})\n",
    "training_data.append({\"class\":\"greetings\", \"sentence\":\"How are you doing?\"})\n",
    "training_data.append({\"class\":\"greetings\", \"sentence\":\"have a nice day\"})\n",
    "training_data.append({\"class\":\"greetings\", \"sentence\":\"good morning.\"})\n",
    "\n",
    "training_data.append({\"class\":\"where_is\", \"sentence\":\"Where is narkel bagan\"})\n",
    "training_data.append({\"class\":\"where_is\", \"sentence\":\"show me the way to sasta sundar\"})\n",
    "training_data.append({\"class\":\"where_is\", \"sentence\":\"where is the staircase\"})\n",
    "training_data.append({\"class\":\"where_is\", \"sentence\":\"give me the direction to the parking lot\"})\n",
    "print (\"%s sentences in training data\" % len(training_data))\n",
    "\n",
    "#organizing our data structures for documents , classes, words\n",
    "\n",
    "words=[]\n",
    "classes=[]\n",
    "documents=[]\n",
    "ignore_words=['?']\n",
    "\n",
    "#loop through each sentence in our training data\n",
    "for pattern in training_data:\n",
    "    #tokenize in each word in the sentence\n",
    "    w=nltk.word_tokenize(pattern['sentence'])\n",
    "    #add to our words list\n",
    "    words.extend(w)\n",
    "    #add to documents in our corpus\n",
    "    documents.append((w,pattern['class']))\n",
    "    #add to our classes list\n",
    "    if pattern['class'] not in classes:\n",
    "        classes.append(pattern['class'])\n",
    "        \n",
    "#stem and lower each word and remove duplicate\n",
    "words=[stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "\n",
    "words=list(set(words))\n",
    "\n",
    "#remove duplicates\n",
    "classes=list(set(classes))\n",
    "\n",
    "print(len(documents),\" documents\")\n",
    "print(len(classes), \" classes\", classes)\n",
    "print(len(words),\" unique stemmed words\", words)\n",
    "\n",
    "\n",
    "#create our training data\n",
    "training=[]\n",
    "output=[]\n",
    "#create an empty array for our output\n",
    "output_empty=[0]*len(classes)\n",
    "\n",
    "#training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    #initialize our bag of words\n",
    "    bag=[]\n",
    "    #list of tokenized words for the pattern\n",
    "    pattern_words=doc[0]\n",
    "    #stem each word\n",
    "    pattern_words=[stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    #create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "    training.append(bag)\n",
    "    #output is a 0 for each tag and 1 for current tag\n",
    "    output_row=list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    output.append(output_row)\n",
    "    \n",
    "#sample training/output\n",
    "i=0\n",
    "w=documents[i][0]\n",
    "print ([stemmer.stem(word.lower()) for word in w])\n",
    "print (training[i])\n",
    "print (output[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi  i want to make a restaurant reservation'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'hi , i want to make a restaurant reservation .'\n",
    "re.sub(\"[\\'.,#!?:-]\", '', sentence).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
